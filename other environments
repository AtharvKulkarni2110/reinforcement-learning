Here's a complete and organized `README.md` file based on your request:

---

# Reinforcement Learning Project

This repository contains implementations of both model-based and model-free reinforcement learning algorithms in various environments such as **Frozen Lake** and **MiniGrid Empty Space**. The primary goal is to explore dynamic programming and model-free control algorithms in these environments and analyze their performance.

## Tasks Covered

### Task 1: Frozen Lake Environment using Dynamic Programming

- **Algorithms**:
  - Policy Iteration
  - Value Iteration
- **Environment Documentation**: [Frozen Lake Environment](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)

** 1)Observation Space-**

 Consider a 4x4 map, then env.observation_space=discrete(16)
 here each state represents a whole number till 15.
 Further custom maps can be defined and each state can be labeled as either S (start) or 
 F(Frozen), H(Hole) or G(Goal).
 By default mapping, S is at 0 and G is at 15.

2)Action space -
 there are 4 actions possible:
    0: Move left
    1: Move down
    2: Move right
    3: Move up

3)Rewards-
    1 , reaching goal
    0 , otherwise

### Task 2: Model-Free Control Algorithms in MiniGrid Empty Space

- **Algorithms**:
  - Monte Carlo
  - Sarsa
  - Sarsa(λ)
  - Q-Learning

- **Environment Documentation**: [MiniGrid Empty Space](https://minigrid.farama.org/environments/minigrid/EmptyEnv/)

 **1) Observation Space -**

        env.observation_space = { 'image': Box(0,255,(7,7,3),uint8) , 'direction':Discrete(4) ,
        'mission': "Get to the green square" }
        Significance of key image :
        It represents the information that the agent can perceive at any grid cell.
        The (7,7,3) matrix: at any grid cell the agent can be visualized at the center of the 
        7x7 grid ( any of those 7x7 grid cells may be behind the walls i.e. can be outside 
        the environment), these 7x7 cells can be termed as 'Vision of Agent' (at any given
        state, number of states about which the agent holds information). * The 3 in (7x7x3) 
        is for the color intensity of a particular grid cell ( among that 7x7) There are 3 
        elements in the matrix R, G, B each containing intensity in the range [0,255].

          Significance of key direction:
        The value of this can be any number in the interval [0,3] which basically tends to denote 
        the direction the agent is facing at any state.
        * 0 : Right * 1 : Down * 2 : Left * 3 : Up

          Significance of key Mission:
        It contains a string value that represents the goal of the agent.
    
 **2) Action space-**

         Actions we can take are -
           0: Turn Left
           1: Turn Right
           2: Move Forward



## Results

### **Frozen Lake (Dynamic Programming)**:
- Policy Iteration and Value Iteration algorithms both converge relatively quickly, as the environment is simple and the state space is small (4x4 grid with 16 states).
- These algorithms explore the environment efficiently due to the known model of the environment (model-based RL).

### **MiniGrid Empty Space (Model-Free Control Algorithms)**:
- From the provided graphs, **Sarsa(λ)** converges faster than **Q-learning** due to its ability to balance exploration and exploitation more effectively.
- **Q-learning**, while effective, explores the environment more extensively and takes more steps to converge.
- **Monte Carlo** and **Sarsa** methods both perform well, but Sarsa(λ) shows better convergence under similar hyperparameter settings.


